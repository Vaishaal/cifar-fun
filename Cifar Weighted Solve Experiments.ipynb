{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5167988e-f347-496e-abe3-310656310669"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K20c (CNMeM is enabled with initial size: 98.0% of memory, CuDNN 5005)\n",
      "/data/vaishaal/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your CuDNN version is more recent then Theano. If you see problems, try updating Theano or downgrading CuDNN to version 4.\n",
      "  warnings.warn(warn)\n",
      "/data/vaishaal/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cifar_gpu_fun\n",
    "reload(cifar_gpu_fun)\n",
    "from cifar_gpu_fun import *\n",
    "import bcd\n",
    "reload(bcd)\n",
    "from robust_probability import ctype_robust_probability\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c640590a-a864-4de1-bdac-8f04d9c8def4"
    }
   },
   "source": [
    "#### Define Some Constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "5ee9705d-3093-4d0f-bd82-0e3dfcd14555"
    }
   },
   "outputs": [],
   "source": [
    "NUM_FEATURE_BATCHES=2\n",
    "RHO = [0.001, 0.01, 0.1]\n",
    "DATA_BATCH_SIZE=(1280)\n",
    "FEATURE_BATCH_SIZE=(1024)\n",
    "NUM_TRAIN = 50000\n",
    "NUM_TEST = 10000\n",
    "NUM_CLASSES = 10\n",
    "NUM_FEATURES = FEATURE_BATCH_SIZE * NUM_FEATURE_BATCHES * 8\n",
    "POOL_TYPE ='avg'\n",
    "FILTER_GEN ='empirical'\n",
    "BANDWIDTH = 1.0\n",
    "LAMBDAS = [0.01, 0.1, 0.34, 1.0, 3.37, 10.0, 100.0, 337.0, 1000.0][::-1]\n",
    "CUDA_CONVNET = True\n",
    "SCALE = 55.0\n",
    "BIAS = 1.25\n",
    "MIN_VAR_TOL = 1e-4\n",
    "TOT_FEAT = FEATURE_BATCH_SIZE*NUM_FEATURE_BATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d4e10ddf-1e84-46e6-827c-649e44ec1642"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "(XTrain, labelsTrain), (XTest, labelsTest) = load_cifar_processed()\n",
    "patches = patchify_all_imgs(XTrain, (6,6), pad=False)\n",
    "if FILTER_GEN == 'gaussian':\n",
    "       filter_gen = make_gaussian_filter_gen(1.0)\n",
    "elif FILTER_GEN == 'empirical':\n",
    "        filter_gen = make_empirical_filter_gen(patches, labelsTrain)\n",
    "else:\n",
    "    raise Exception('Unknown FILTER_GEN value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out Shape  2 x 2 x 4096\n",
      "FEATURE BATCH # 0 DATA BATCH # 0  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 1  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 2  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 3  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 4  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 5  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 6  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 7  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 8  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 9  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 10  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 11  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 12  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 13  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 14  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 15  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 16  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 17  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 18  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 19  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 20  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 21  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 22  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 23  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 24  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 25  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 26  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 27  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 28  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 29  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 30  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 31  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 32  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 33  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 34  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 35  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 36  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 37  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 38  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 39  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 40  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 41  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 42  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 43  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 44  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 45  SIZE IS  1280\n",
      "FEATURE BATCH # 0 DATA BATCH # 46  SIZE IS  1120\n",
      "FEATURE BATCH # 1 DATA BATCH # 0  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 1  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 2  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 3  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 4  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 5  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 6  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 7  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 8  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 9  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 10  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 11  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 12  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 13  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 14  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 15  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 16  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 17  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 18  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 19  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 20  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 21  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 22  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 23  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 24  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 25  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 26  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 27  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 28  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 29  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 30  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 31  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 32  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 33  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 34  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 35  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 36  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 37  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 38  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 39  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 40  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 41  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 42  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 43  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 44  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 45  SIZE IS  1280\n",
      "FEATURE BATCH # 1 DATA BATCH # 46  SIZE IS  1120\n"
     ]
    }
   ],
   "source": [
    "# Featurize here\n",
    "X = np.vstack((XTrain, XTest))\n",
    "(XFeaturized, filters) = conv(X, filter_gen, 1024, NUM_FEATURE_BATCHES, 1280, CUDA_CONVNET, pool_size=14, symmetric_relu=True)\n",
    "XFeaturizedTrain = XFeaturized[:50000,:,:,:]                                                                                                                                             \n",
    "XFeaturizedTest = XFeaturized[50000:,:,:,:]\n",
    "\n",
    "XFeaturizedTrain = XFeaturizedTrain.reshape(50000, -1)\n",
    "XFeaturizedTest = XFeaturizedTest.reshape(10000, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XFeaturizedTrain.dot(XFeaturizedTrain.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set weights here\n",
    "W = np.ones(XTrain.shape[0])[:, np.newaxis]\n",
    "\n",
    "#W = np.random.dirichlet(2*np.ones(XTrain.shape[0]))[:, np.newaxis] * XTrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cifar_gpu_fun.py:201: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if (W == None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X SHAPE  (50000, 16384)\n",
      "Computing XTX\n",
      "Done Computing XTX\n",
      "CPU times: user 6min 16s, sys: 2min 11s, total: 8min 27s\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "# Solve weighted least squares here (in primal)\n",
    "%time model = learnPrimal(XFeaturizedTrain, labelsTrain, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on training set\n",
    "YHat = XFeaturizedTrain.dot(model)\n",
    "labelsPredTrain = np.argmax(YHat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  721935.18898\n",
      "Training Accuracy  0.97116\n"
     ]
    }
   ],
   "source": [
    "# Calculate training loss and training accuracy\n",
    "YTrain = np.eye(max(labelsTrain) + 1)[labelsTrain]\n",
    "loss = np.linalg.norm(YHat - YTrain)**2 + LAMBDAS[0]*(np.linalg.norm(model)**2)\n",
    "train_acc = metrics.accuracy_score(labelsTrain, labelsPredTrain)\n",
    "print \"Training Loss: \", loss\n",
    "print \"Training Accuracy \", train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.7323\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "YHatTest = XFeaturizedTest.dot(model)\n",
    "labelsPredTest = np.argmax(YHatTest, axis=1)\n",
    "test_acc = metrics.accuracy_score(labelsTest, labelsPredTest)\n",
    "print \"Testing Accuracy: \", test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49999.99999999992"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Robust solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.0\n",
      "0.001\n",
      "Iteration:  0\n",
      "X SHAPE  (50000, 16384)\n",
      "Computing XTX\n",
      "Done Computing XTX\n",
      "[[ 0.96756863]\n",
      " [ 1.03388408]\n",
      " [ 0.96529532]\n",
      " ..., \n",
      " [ 1.01355031]\n",
      " [ 1.05421067]\n",
      " [ 0.94572711]]\n",
      "Training Loss:  29152.9456496\n",
      "Training Accuracy  0.75166\n",
      "Testing Accuracy:  0.7012\n",
      "Iteration:  1\n",
      "X SHAPE  (50000, 16384)\n",
      "Computing XTX\n",
      "Done Computing XTX\n",
      "[[ 0.96719032]\n",
      " [ 1.03418515]\n",
      " [ 0.96485671]\n",
      " ..., \n",
      " [ 1.01436895]\n",
      " [ 1.05409542]\n",
      " [ 0.94549088]]\n",
      "Training Loss:  29163.1204308\n",
      "Training Accuracy  0.74838\n",
      "Testing Accuracy:  0.6981\n",
      "Iteration:  2\n",
      "X SHAPE  (50000, 16384)\n",
      "Computing XTX\n"
     ]
    }
   ],
   "source": [
    "# Start weights\n",
    "N = XTrain.shape[0]\n",
    "W = np.ones(N)[:, np.newaxis]\n",
    "\n",
    "\n",
    "for lambda_ in LAMBDAS:\n",
    "    for rho_ in RHO:\n",
    "        print lambda_\n",
    "        print rho_\n",
    "        for i in xrange(5):\n",
    "            print \"Iteration: \", i\n",
    "            # This should converge quickly right?\n",
    "\n",
    "            model = learnPrimal(XFeaturizedTrain, labelsTrain, W, reg=lambda_)\n",
    "            # Evaluate on training set\n",
    "            YHat = XFeaturizedTrain.dot(model)\n",
    "            labelsPredTrain = np.argmax(YHat, axis=1)\n",
    "\n",
    "            # Calculate training loss and training accuracy\n",
    "            YTrain = np.eye(max(labelsTrain) + 1)[labelsTrain]\n",
    "            losses = np.square(YHat - YTrain).sum(axis=1)\n",
    "            W = N*ctype_robust_probability.robust_probability(-losses, rho_)[:, np.newaxis]\n",
    "            print W\n",
    "            loss = np.linalg.norm(YHat - YTrain)**2 + LAMBDAS[0]*(np.linalg.norm(model)**2)\n",
    "            train_acc = metrics.accuracy_score(labelsTrain, labelsPredTrain)\n",
    "            print \"Training Loss: \", loss\n",
    "            print \"Training Accuracy \", train_acc\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            YHatTest = XFeaturizedTest.dot(model)\n",
    "            labelsPredTest = np.argmax(YHatTest, axis=1)\n",
    "            test_acc = metrics.accuracy_score(labelsTest, labelsPredTest)\n",
    "            print \"Testing Accuracy: \", test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {
    "691889e3-b7c6-4914-802f-c045471bb148": {
     "id": "691889e3-b7c6-4914-802f-c045471bb148",
     "prev": null,
     "regions": {
      "d473ec79-b583-4b90-8f0c-ca04540a8b42": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "d473ec79-b583-4b90-8f0c-ca04540a8b42"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
